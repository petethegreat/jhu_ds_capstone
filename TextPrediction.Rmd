---
title: "JHU Capstone Project - text prediction"
output:
  html_document:
    theme: "cerulean" 
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "all"      } 
  }
});
</script>

# Introduction
I've added some notes on my work here. 

# N-Gram creation
N-Grams are created using the tau package, while the tm package was used to create and transform the corpus. Processing the corpus was very resource intensive given my available computing power (I ran R through a linux VM with up to 5 GB of memory). I initially tried using the tm package to create ngrams, using tokenizers created through RWeka for bigrams and higher. I tried a few approaches here, but none performed very well. 

The code I ended up using for ngram creation is in n2.R


  * split the corpus into train, validate, and test samples (80%/10%/10%), one each for twitter, news and blogs
  * split each training set into 40 temporary input files
  * process each temporary input file. The input is cleaned (chracters are converted to utf-8, lower case, with  numbers, punctuation and extra white space removed) and then ngrams are created using the '''textcnt''' function from tau. The ngram counts are then written to csv (one for each temp input file)
  * Each ngram csv is read into a data table, then merged with a total count (using an outer join). The total is then written to csv.

  In the end, I split the training set into 40 temporary files, and used 30 of these (so 75% of 80% of the corpus was used to train the model.) All observed words were kept for 1-grams, bigrams and trigrams were required to have a total count of at least 2 (or were discarded), while 4 and 5 grams were kept only if the total count was 5 or higher. File sizes for the csvs containing the file ngram data are given below


|file| size (MB) |
|--------|-----|
|cleaned_counts_n1.csv| 8.8 |
|cleaned_counts_n2.csv| 52 |
|cleaned_counts_n3.csv| 106 |
|cleaned_counts_n4.csv| 12 |
|cleaned_counts_n5.csv| 4.1 |

## Model

An interpolated Kneser-Ney model is used for prediction. The next-word probabilities are computed based on n-gram frequencies, such that 

\begin{equation}
P(w_i|w_{i-n+1}^{i-1}) = \frac{C(w_{i-n+1}^{i-1} \, w_i) -d}{C(w_{i-n+1}^{i-1})}
\end{equation}

Where $P(w_i|w_{i-n+1}^{i-1})$ is the probability of the next word, $w_i$, preceeding the $n$-word history $w_{i-n+1}^{i-1}$, $C(w_{i-n+1}^{i-1} \, w_i)$ is the count of the $n+1$ gram $w_{i-n+1}^{i-1} \, w_i$, $C(w_{i-n+1}^{i-1})$ is the count of the history, and $d$ is a discount value. Essentially, the probability of the candidate $w_i$ is the relative number of times that the n-gram formed by the $n$ word history is completed by $w_i$. The discount reduces these probabilities, such that they will not sum to 1.

The remaining probability mass is based on continuation. The n-gram probability above gives a good probability estimate based on previously seen contexts, but it could be that the ideal next word will be occuring in a context that was not seen in our training corpus. In this case, we form the probability based on continuation counts, that is, the number of unique n-grams that are completed by the candidate word. If a word occurs in a lot of different contexts it may be a good match for the next word, even if we haven't observed it in that context before.

The word probability based on continuation is given by
$$ P(w_i|w_{i-n+1}^{i-1}) = \frac{N_{1+}(\dot w_i) -d}{N_{1+}(\dot\,\dot)} $$,
where $N_{1+}(\dot w_i)$ is the number of unique n-grams (with at least one count) that end with $w_i$, and $N_{1+}(\dot\,\dot)$ is the total number of unique n-grams with at least one count. 

The probability computation is carried out in the predictCleaned function of model.R -
  - Try searching for the _n-1_ words of the history in the highest order n-grams available (n=5 in this case). If no match is found, try the next highest order (4)
  - Once the (some amount) of the history has been found, 




The same transformations (lower-case utf8, digits and punctuation removed) that were applied to the training corpus are performed on the input string. The 

## References

  * draft version of "[Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/4.pdf)", chapter 4, by Daniel Jurafsky and James Martin, while researching n-grams and NLP. 
  * Chen, S. and Goodman, J., "An empirical study of smoothing techniques for language modeling", _Computer speech and Language (1999)_, __13__.







 
