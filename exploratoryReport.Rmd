---
title: "Swiftkey NLP project - Exploratory Analysis"
author: "Peter Thompson"
output:
  html_document:
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  pdf_document:
    toc: true
---

===============================

## Loading the data


Can get some quick statistics on the input files using the ```wc``` command ( ```wc -l -w -L```) on linux/unix. This information is printed below.

| file | lines | words | max line length |
|:-----|------:|------:|----------------:|
|en_US.blogs.txt | 899288 | 37334117 | 40833 |
|en_US.news.txt | 1010242 | 34365936 | 11384 |
|en_US.twitter.txt | 2360148 | 30373559 | 173 |


Will load this data into R, using only a small subset (1%) to start with. The tm package will be used to build a corpus, which treats each line as a document.


```{r,loadData,cache=TRUE,warning=FALSE}

library(tm)
library(RWeka)

blogs<-readLines('./data/final/en_US/en_US.blogs.txt')
news<-readLines('./data/final/en_US/en_US.news.txt')
tweets<-readLines('./data/final/en_US/en_US.twitter.txt')

set.seed(97)
frac<-0.01
textData<-c(
    sample(blogs,floor(frac*length(blogs))),
    sample(news,floor(frac*length(news))),
    sample(tweets,floor(frac*length(tweets)))
    )

rm(blogs,news,tweets)
mycorpus<-Corpus(VectorSource(textData))
```

We are interested in the words contained in the corpus. Will do some cleaning to convert all upper case characters to lower case, remove numbers, punctuation, and additional whitespace.

```{r, cleanData, cache=TRUE, dependson='loadData'}
# clean here
mycorpus<-tm_map(mycorpus,tolower)
mycorpus<-tm_map(mycorpus,removeNumbers)
#mycorpus<-tm_map(mycorpus,removeWords, stopwords("english"))
mycorpus<-tm_map(mycorpus,removePunctuation)
mycorpus<-tm_map(mycorpus,stripWhitespace)
```

From the corpus created above, some 1-grams (words) will be built. This is done by constructing a term document matrix, which is a matrix containing word counts per document (documents correspond to columns, words to rows). Total word counts from the corpus can be constructing by taking row sums. the ```TermDocumentMatrix``` function from the tm package uses a one word tokeniser by default, to construct bigrams or trigrams a custom function needs to be created that will tokenise word pairs or triplets (the ```NGramTokenizer``` function from RWeka is good for this).


```{r, makegrams, cache=TRUE, dependson='cleanData'}
#gram2tokeniser<- function(x) NGramTokenizer(x,Weka_control(min = 3, max = 3))
tdm_1gram<-TermDocumentMatrix(mycorpus)#,control=list(tokenize=gram2tokeniser))

inspect(tdm_1gram)
```

## Exploratory Analysis

Initially there were 56,720 unique terms (words) in our corpus. That's a lot, and we probably don't need all of them. We can trim this by keeping only those terms that are less than 99.9% sparse, that is, only those terms that occur in at least 0.1% of our documents (lines).

```{r dimensions, cache=TRUE, dependson='makegrams',warning=FALSE}
dim(tdm_1gram)

tdm_1gram<-removeSparseTerms(tdm_1gram,0.999)
dim(tdm_1gram)
```

After removing sparse terms, the term document matrix contains only 2176 unique words.

```{r wordcounts, cache=TRUE, dependson='dimensions'}

wordcounts<-rowSums(as.matrix(tdm_1gram))
sum(wordcounts)
wordfreq<-sort(wordcounts, decreasing=TRUE)

```

There are a total of 584907 distinct words in the corpus (that is, the 1% sampled from the total corpus). A bar chart showing the frequency of the 50 most common words is plotted below.

```{r, output=FALSE,echo=FALSE}
library(ggplot2)
```


```{r wordbars, cache=TRUE, dependson='wordcounts'}
df<-data.frame(wordcount=wordfreq,term=names(wordfreq))
df<-df[order(df$wordcount,decreasing=TRUE)[1:20],]
g<-ggplot(data=df,aes(x=term,weight=wordcount,fill=wordcount)) + 
    geom_bar(position=position_stack(reverse=TRUE)) + 
    scale_x_discrete(limits=rev(df$term)) +
    scale_fill_gradientn(colors=c('blue','red')) +
    labs(x='word',y='count',title='20 most frequent words') +
    coord_flip()
g

```

The most common word is, by far, 'the'. The words 'and' and 'for' are also common. Word frequencies for all (non-sparse) words in the corpus are plotted below.

```{r wordcurve,cache=TRUE,dependson='wordcounts'}

df2<-data.frame(wordcount=wordfreq,term=names(wordfreq),index=seq_along(wordfreq))
h<- ggplot(data=df2,aes(x=index,y=wordcount)) + geom_line(color='purple') + scale_y_log10()
h

```
There is a very sharp drop in word frequency.

```{r wordlength, cache=TRUE, dependson='wordcurve'}

df2$termlength=nchar(as.character(df2$term))

j<-ggplot(data=df2,aes(termlength,weight=wordcount,fill=..count..)) +
    geom_histogram(binwidth=0.5) +
    scale_y_log10() +
    scale_fill_gradientn(colors=c('blue','red')) +
    labs(x='term length',y='count')
j

```

Short words are more common, though the drop as word length increases is fairly steady, in contrast to the sharp drop seen in the word freequency plot.


## Model Considerations

A simple could be made by constructing bigrams (2-grams) of words from the corpus. All bigrams that begin with the ast word of the input could be considered, and the second word of the highest frequency bigram would be used as the prediction. This could be extended to use trigrams by looking at the last two words of the input.

For building the ngrams, memory and computation time may be an issue. Might want to consider setting up R on a free AWS account for some additional computing power.

In cases where the last few words of input are not contained in any of the ngrams built from the training corpus, then smoothing/backoff models may be used. This uses lower order ngrams (e.g. bigrams) to estimate probability in cases where the input word(s) are not present in the highest order ngrams (e.g. trigrams).

Ngram information could be stored in a tree structure using the ```data.tree``` package. The first level of the tree would correspond to all of the words observed in the corpus. Each of these words would then have a child node for each observed bigram, and each of these nodes would have a child for each observed trigram. This will take up a lot of memory, but after the tree is constructed then low frequency nodes could be pruned.

Some context has been lost from the corpus used above, as there is presently no information relating to word positioning in a sentence (all punctuation is removed before computing ngrams). It's possible that this could be retained, but it would make constructing the model more difficult.


